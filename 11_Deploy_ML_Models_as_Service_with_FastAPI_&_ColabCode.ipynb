{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11 - Deploy ML Models as Service with FastAPI & ColabCode ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfkUNsC_T2qy"
      },
      "source": [
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjR3OXrZUWbN"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00z03aC9YVGk"
      },
      "source": [
        "cc = ColabCode(port=12000, code=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFXBGHaNUk3h"
      },
      "source": [
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "  return {\"Hello\": \"World\"}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzWXXJs-UmFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8a56c3-c383-4b8c-c916-dccf81d148b9"
      },
      "source": [
        "cc.run_app(app=app)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [59]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://05b4-35-243-203-231.ngrok.io\" -> \"http://localhost:12000\"\n",
            "INFO:     182.179.148.71:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     119.153.111.198:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE3gaKk7UpEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222750bd-69ee-46f5-a481-5e87b1800c98"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "iris = load_iris()\n",
        "model = GaussianNB()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "model_f = model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model score: \", model.score(X_train, y_train))\n",
        "print(\"Test Accuracy: \", model.score(X_test, y_test))\n",
        "\n",
        "pickle.dump(model_f, open(\"model_gb.pkl\", \"wb\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model score:  0.9481481481481482\n",
            "Test Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sWUbNKGW9ZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbeb2294-3230-4fab-c11f-4b321290329c"
      },
      "source": [
        "%%writefile models.py\n",
        "from pydantic import BaseModel, conlist\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Iris(BaseModel):\n",
        "    data: List[conlist(float, min_items=4, max_items=4)]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMpFW8OAV2r3"
      },
      "source": [
        "import pickle\n",
        "import logging\n",
        "from fastapi import FastAPI\n",
        "from models import Iris\n",
        "\n",
        "app = FastAPI(title=\"ML Models as API on Google Colab\", description=\"with FastAPI and ColabCode\", version=\"1.0\")\n",
        "\n",
        "# # Initialize logging\n",
        "# my_logger = logging.getLogger()\n",
        "# my_logger.setLevel(logging.DEBUG)\n",
        "# logging.basicConfig(level=logging.DEBUG, filename='logs.log')\n",
        "\n",
        "model = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global model\n",
        "    model = pickle.load(open(\"model_gb.pkl\", \"rb\"))\n",
        "\n",
        "@app.post(\"/api\", tags=[\"prediction\"])\n",
        "async def get_predictions(iris: Iris):\n",
        "    try:\n",
        "        data = dict(iris)['data']\n",
        "        print(data)\n",
        "        iris_types = {\n",
        "            0: 'setosa',\n",
        "            1: 'versicolor',\n",
        "            2: 'virginica'\n",
        "        }\n",
        "        prediction = list(map(lambda x: iris_types[x], model.predict(data).tolist()))\n",
        "        log_proba = model.predict_log_proba(data).tolist()\n",
        "        return {\"prediction\": prediction, \"log_proba\": log_proba}\n",
        "    except:\n",
        "        my_logger.error(\"Something went wrong!\")\n",
        "        return {\"prediction\": \"error\"}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFl9Ga5uW5ED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f370127b-1bea-4c51-bac3-b8e1fedd7297"
      },
      "source": [
        "cc.run_app(app=app)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://644a-35-243-203-231.ngrok.io\" -> \"http://localhost:12000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [59]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [59]\n"
          ]
        }
      ]
    }
  ]
}